{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from keras.datasets import mnist\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, layers):  # Define some useful parameters for the network.\n",
        "        self.num_layers = len(layers)\n",
        "        self.layers = layers\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]\n",
        "        self.biases = [np.random.randn(y, 1) for y in layers[1:]]\n",
        "\n",
        "    def SGD(self, train_list, iterations, mini_batch_size, learning_rate, test_list=None):\n",
        "        # Stochastic Gradient Descent (SGD) algorithm. `learning_rate` is the learning rate\n",
        "        # (divided by 60000). If `test_list` is provided, after each iteration of `train_list`,\n",
        "        # the network's accuracy on the 10000 test samples will be evaluated.\n",
        "\n",
        "        n = len(train_list)\n",
        "        if n % mini_batch_size != 0:\n",
        "            print(\"Error: Mini-batch size is not a multiple of 60000.\")\n",
        "            raise SystemExit\n",
        "        if test_list:\n",
        "            n_test = len(test_list)\n",
        "        for j in range(iterations):\n",
        "            random.shuffle(train_list)  # Shuffle the training list.\n",
        "            mini_batches = [train_list[k:k + mini_batch_size]  # Define the mini-batches.\n",
        "                            for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:  # Update each mini-batch separately.\n",
        "                self.update_mini_batch(mini_batch, learning_rate)\n",
        "            if test_list:\n",
        "                accuracy = self.evaluate(test_list)  # Evaluate the network if test_list is provided.\n",
        "                print(f\"Iteration {j + 1}: {accuracy}/{n_test} correct.\")\n",
        "            else:\n",
        "                print(f\"Iteration {j} completed.\")\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, learning_rate):\n",
        "        # This function updates the mini-batches obtained in SGD(). It updates the\n",
        "        # network's weights and biases.\n",
        "\n",
        "        bias_grad = [np.zeros(bias.shape) for bias in self.biases]  # Initialize gradient vectors.\n",
        "        weight_grad = [np.zeros(weight.shape) for weight in self.weights]  # For weights and biases.\n",
        "        for x, y in mini_batch:\n",
        "            bias_delta_grad, weight_delta_grad = self.backpropagation(x, y)  # Backpropagation.\n",
        "            bias_grad = [bias + bias_derivative for bias, bias_derivative\n",
        "                         in zip(bias_grad, bias_delta_grad)]\n",
        "            weight_grad = [weight + weight_derivative for weight, weight_derivative\n",
        "                           in zip(weight_grad, weight_delta_grad)]\n",
        "        self.weights = [weight - (learning_rate / len(mini_batch)) * weight_gradient\n",
        "                        for weight, weight_gradient in zip(self.weights, weight_grad)]\n",
        "        self.biases = [bias - (learning_rate / len(mini_batch)) * bias_gradient\n",
        "                       for bias, bias_gradient in zip(self.biases, bias_grad)]\n",
        "\n",
        "    def forward_propagation(self, A):\n",
        "        # Forward propagation algorithm, returns the activations of the last layer.\n",
        "\n",
        "        for bias, weight in zip(self.biases, self.weights):\n",
        "            A = sigmoid(np.dot(weight, A) + bias)\n",
        "        return A\n",
        "\n",
        "    def backpropagation(self, x, y):\n",
        "        # This function returns a tuple (bias_grad, weight_grad), which is the gradient\n",
        "        # of the cost function.\n",
        "\n",
        "        bias_grad = [np.zeros(bias.shape) for bias in self.biases]  # Initialize gradient vectors.\n",
        "        weight_grad = [np.zeros(weight.shape) for weight in self.weights]  # For weights and biases.\n",
        "\n",
        "        # Forward propagation to save Z values and activations.\n",
        "        activation = x\n",
        "        saved_activations = [x]\n",
        "        saved_z = []\n",
        "        for bias, weight in zip(self.biases, self.weights):\n",
        "            z = np.dot(weight, activation) + bias\n",
        "            saved_z.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            saved_activations.append(activation)\n",
        "\n",
        "        # Backward propagation starts here.\n",
        "        delta = (saved_activations[-1] - y) * sigmoid_derivative(saved_z[-1])\n",
        "        bias_grad[-1] = delta\n",
        "        weight_grad[-1] = np.dot(delta, saved_activations[-2].transpose())\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = saved_z[-l]\n",
        "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sigmoid_derivative(z)\n",
        "            bias_grad[-l] = delta\n",
        "            weight_grad[-l] = np.dot(delta, saved_activations[-l - 1].transpose())\n",
        "\n",
        "        return bias_grad, weight_grad\n",
        "\n",
        "    def evaluate(self, test_list):\n",
        "        # This function calculates the total number of correct predictions on the 10000\n",
        "        # test images. `test_results` is a list of 10000 tuples (x, y) where x is the\n",
        "        # predicted value and y is the actual label.\n",
        "\n",
        "        test_results = [(np.argmax(self.forward_propagation(x)), y)\n",
        "                        for (x, y) in test_list]\n",
        "        return sum(int(x == y) for x, y in test_results)\n",
        "\n",
        "# Define some useful functions outside the network.\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def prepare_training_list(train_x, train_y):\n",
        "    # Prepare the training data as a list. Each element is a tuple (x, y), where\n",
        "    # x is a vector of pixel values, and y is a vector representing the label.\n",
        "\n",
        "    train_x = train_x / 255  # Normalize.\n",
        "    train_list = []\n",
        "\n",
        "    for i in range(60000):\n",
        "        x = np.zeros((784, 1))\n",
        "        for j in range(28):\n",
        "            for k in range(28):\n",
        "                x[k + 28 * j] = train_x[i, j, k]\n",
        "\n",
        "        y = np.zeros((10, 1))\n",
        "        digit = train_y[i]\n",
        "        y[digit] = 1\n",
        "\n",
        "        pair = (x, y)\n",
        "        train_list.append(pair)\n",
        "\n",
        "    return train_list\n",
        "\n",
        "def prepare_test_list(test_x, test_y):\n",
        "    # This function is similar to the previous one, except that the second element\n",
        "    # of each tuple is the actual label instead of a one-hot vector.\n",
        "\n",
        "    test_x = test_x / 255\n",
        "    test_list = []\n",
        "\n",
        "    for i in range(10000):\n",
        "        x = np.zeros((784, 1))\n",
        "        for j in range(28):\n",
        "            for k in range(28):\n",
        "                x[k + 28 * j] = test_x[i, j, k]\n",
        "\n",
        "        digit = test_y[i]\n",
        "\n",
        "        pair = (x, digit)\n",
        "        test_list.append(pair)\n",
        "\n",
        "    return test_list\n",
        "\n",
        "# Load the data.\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "# Prepare the data for the network.\n",
        "train_list = prepare_training_list(train_x, train_y)\n",
        "test_list = prepare_test_list(test_x, test_y)\n",
        "\n",
        "# Define the network.\n",
        "network = Network([784, 50, 10])\n",
        "\n",
        "# Train and evaluate the network.\n",
        "network.SGD(train_list, 30, 10, 3, test_list=test_list)\n"
      ],
      "metadata": {
        "id": "m15666fYS3fb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}